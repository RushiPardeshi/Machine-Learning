# -*- coding: utf-8 -*-
"""ML_ass1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rsvNwtFUreiPabqgZlJRHtmknPHf1SqL
"""

import numpy as np
import pandas as pd

df = pd.read_csv("https://raw.githubusercontent.com/RushiPardeshi/Machine-Learning/main/owid-covid-data%20(1).csv")
new_data = df[df['location']=='India']
# new_data

da = pd.read_csv('https://raw.githubusercontent.com/RushiPardeshi/Machine-Learning/main/changes-visitors-covid.csv')
indian_visitor = da[da['Entity']=='India']
# indian_visitor

target = new_data[['date','new_cases']]
target = target.rename({'date':'Day'},axis=1)
# target

final = pd.merge(indian_visitor,target,on='Day')
final = final.drop(['Entity','Code','Day'],axis=1)
final

# split the data in 60:20:20 for train:valid:test dataset
train_size = 0.8
valid_size = 0.2

train_index = int(len(final)*train_size)

# # First we need to sort the dataset by the desired column 
# final.sort_values(by = 'cases', ascending=True, inplace=True)

final_train = final[0:train_index]
final_rem = final[train_index:]

valid_index = int(len(final)*valid_size)

final_valid = final[train_index:train_index+valid_index]
final_test = final[train_index+valid_index:]

X_train, y_train = final_train.drop(columns = 'new_cases').copy(), final_train['new_cases'].copy()
X_valid, y_valid = final_valid.drop(columns = 'new_cases').copy(), final_valid['new_cases'].copy()
X_test, y_test = final_test.drop(columns = 'new_cases').copy(), final_test['new_cases'].copy()
        
# print(X_train.shape), print(y_train.shape)
# print(X_valid.shape), print(y_valid.shape)
# print(X_test.shape), print(y_test.shape)
# X = final.iloc[:, :-1].values
# Y = final.iloc[:, -1].values.reshape(-1,1)
# from sklearn.model_selection import train_test_split
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=41)
X_train

#Node Class

class Node():
  def __init__(self, feature_index=None,threshold=None, left=None, right=None, var_red =None, value=None):
    self.feature_index = feature_index # index of max info_gain
    self.threshold = threshold 
    self.left = left
    self.right = right
    self.var_red = var_red # value at which we get info gain
    
    # for leaf node
    self.value = value

#Tree class
class DecisionTreeRegressor():
  def __init__(self,min_samples_split=2, max_depth=2):
    #initialize the root of the tree
    self.root=None
    #stop condition
    self.min_samples_split = min_samples_split
    self.max_depth = max_depth

  def build_tree(self, final_train_dataset, curr_depth=0):
    X_train, y_train = final_train_dataset[:,:-1],final_train_dataset[:,-1]
    num_samples, num_features = np.shape(X_train)
    best_split = {}
    # split until stopping conditions are met
    if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:
      # find best split
      best_split = self.get_best_split(final_train_dataset,num_samples,num_features)

      if best_split["info_gain"]>0:
        left_subtree = self.build_tree(best_split["dataset_left"],curr_depth+1)
        right_subtree = self.build_tree(best_split["dataset_right"],curr_depth+1)

        return Node(best_split["feature_index"],best_split["threshold"],left_subtree,right_subtree,best_split["info_gain"])

  #leaf node
    leaf_value = self.calculate_leaf_value(y_train)   #After the tree is recursively built, calculating and returning the leaf nodes.

    return Node(value=leaf_value)    




  def get_best_split(self, dataset,num_samples,num_features):
    best_split={} #dictionary to store the best split
    max_gain = -float("inf") # minimum value

    for feature_index in range(num_features):
      feature_values = dataset[:,feature_index] # selection of a feature
      possible_thresholds = np.unique(feature_values)
      for threshold in possible_thresholds:
        #get left and right data set
        dataset_left, dataset_right = self.split(dataset, feature_index, threshold)
        if len(dataset_left)>0 and len(dataset_right)>0:
          y,left_y,right_y = dataset[:,-1], dataset_left[:,-1], dataset_right[:,-1]
          #compute_information_gain
          curr_gain = self.variance_reduction(y,left_y,right_y)
          if curr_gain > max_gain:
            best_split["feature_index"] = feature_index
            best_split["threshold"] = threshold
            best_split["dataset_left"] = dataset_left
            best_split["dataset_right"] = dataset_right
            best_split["info_gain"] = curr_gain
            max_gain = curr_gain

    return best_split

    
  def split(self, dataset, feature_index, threshold):
        ''' function to split the data '''
        
        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])
        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])
        return dataset_left, dataset_right
    
  def variance_reduction(self, parent, l_child, r_child):
        ''' function to compute variance reduction '''
        
        weight_l = len(l_child) / len(parent)
        weight_r = len(r_child) / len(parent)
        reduction = np.var(parent) - (weight_l * np.var(l_child) + weight_r * np.var(r_child))
        return reduction
    
  def calculate_leaf_value(self, Y):
        ''' function to compute leaf node '''
        
        val = np.mean(Y)
        return val
                
  def print_tree(self, tree=None, indent=" "):
        ''' function to print the tree '''
        
        if not tree:
            tree = self.root

        if tree.value is not None:
            print(tree.value)

        else:
            print("X_"+str(tree.feature_index), "<=", tree.threshold, "?", tree.var_red)
            print("%sleft:" % (indent), end="")
            self.print_tree(tree.left, indent + indent)
            print("%sright:" % (indent), end="")
            self.print_tree(tree.right, indent + indent)
    
  def fit(self, X, Y):
        ''' function to train the tree '''
        
        y_train_ = y_train[:,np.newaxis] # 282,1
        dataset = np.concatenate((X_train, y_train_), axis = 1)
        self.root = self.build_tree(dataset)
        
  def make_prediction(self, x, tree):
        ''' function to predict new dataset '''
        # print(x)
        if tree.value!=None: 
          return tree.value
        feature_val = x[tree.feature_index]
        if feature_val <= tree.threshold:
          return self.make_prediction(x, tree.left)
        else:
            return self.make_prediction(x, tree.right)
    
  def predict(self, X):
        ''' function to predict a single data point '''
        # print(np.shape(X))

        # predictions = [self.make_prediction(ax, self.root) for ax in X]
        predictions = [self.make_prediction(X_test.loc[ax], self.root) for ax in X_test.index]
        return predictions

#make decision tree
regressor = DecisionTreeRegressor(min_samples_split=4, max_depth=4)
regressor.fit(X_train, y_train)
regressor.print_tree()

#test the model
#ID3-A RMSE root mean squared error
Y_pred = regressor.predict(X_test) 
from sklearn.metrics import mean_squared_error
np.sqrt(mean_squared_error(y_test, Y_pred))

# Y_pred = regressor.predict(X_train)
# np.sqrt(np.sum(((y_test- Y_pred)**2)/len(y_test)))
# Y_pred, y_test

#cart : RSS error function

Y_pred2 = regressor.predict(X_test)
np.sum((y_test - np.mean(Y_pred2)))/len(y_test)

import matplotlib.pyplot as plt
import numpy as np
class K_Means:
    def _init_(self, k=2, tol=0.001, max_iter=300):
        self.k = k
        self.tol = tol
        self.max_iter = max_iter

    def fit(self,data):

        self.centroids = {}

        for i in range(self.k):
            self.centroids[i] = data[i]

        for i in range(self.max_iter):
            self.classifications = {}

            for i in range(self.k):
                self.classifications[i] = []

            for featureset in data:
                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]
                classification = distances.index(min(distances))
                self.classifications[classification].append(featureset)

            prev_centroids = dict(self.centroids)

            for classification in self.classifications:
                self.centroids[classification] = np.average(self.classifications[classification],axis=0)

            optimized = True

            for c in self.centroids:
                original_centroid = prev_centroids[c]
                current_centroid = self.centroids[c]
                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:
                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))
                    optimized = False

            if optimized:
                break

    def predict(self,data):
        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]
        classification = distances.index(min(distances))
        return classification

from scipy import stats
x=final_train.iloc[:,[0,6]].values

x=x[(np.abs(stats.zscore(x)) < 3).all(axis=1)]

import numpy as np
import pandas as pd
from copy import deepcopy
import matplotlib.pyplot as plt

def euclidean(a,b):
	return abs(np.linalg.norm(a-b))

def main():
    #Step 1: Choosing random value of k centroids from the values present in dataset
    #2d array with 2 centroids from x randomly,k=2
    centroid=x[np.random.choice(x.shape[0],3, replace=False)]
    
    print("Centroids are:",centroid)
    
    total=x.shape
    distance_1=np.zeros(total[0])
    distance_2=np.zeros(total[0])
    belongs_to=np.zeros(total[0])
    c_old=np.zeros(centroid.shape)
    error=euclidean(centroid,c_old)
    mean=np.zeros(centroid.shape)
    iterator=0
    
    #Step 2: Calculate euclidean distance for each point with each centroid 
    while error!=0:
        print("Iteration",iterator+1,":")
        for i in range(total[0]):
            distance_1[i]=euclidean(x[i],centroid[0])
            #print("Distance of point",x[i],"with centroid 1:",distance_1[i])
            distance_2[i]=euclidean(x[i],centroid[1])
            #print("Distance of point",x[i],"with centroid 2:",distance_2[i])
            
            #Step 3: Comparing the distances and assigning clusters
            if (distance_1[i]<distance_2[i]):
                belongs_to[i]=0
                #print("Point",x[i]," is in cluster 1.")
            if (distance_1[i]>distance_2[i]):
                belongs_to[i]=1
                #print("Point",x[i]," is in cluster 2.")
                
        #Step 4: Taking mean and repeat
        c_old=deepcopy(centroid)
        for i in range(len(belongs_to)):
            if belongs_to[i]==0:
                mean[0][0]=np.mean(x[i][0])
                mean[0][1]=np.mean(x[i][1])
            else:
                continue
        
        print("New Centroid for cluster 1:",mean[0])
        
        for i in range(len(belongs_to)):
            if belongs_to[i]==1:
                mean[1][0]=np.mean(x[i][0])
                mean[1][1]=np.mean(x[i][1])
            else:
                continue
        
        print("New Centroid for cluster 2:",mean[1])
        
        centroid[0]=mean[0]
        centroid[1]=mean[1]
        error=euclidean(centroid,c_old)
        iterator+=1
        if error==0:
            print("Same centroids again!")
    
    #plot the clustered points
    plt.rcParams['figure.figsize'] = (16, 9)
    plt.style.use('ggplot')
    colors = ['r', 'g']
    fig, ax = plt.subplots()
    for i in range(2):
        points = np.array([x[j] for j in range(len(x)) if belongs_to[j] == i])
        ax.scatter(points[:, 0], points[:, 1], s=7, c=colors[i])
    ax.scatter(centroid[:, 0], centroid[:, 1], marker='*', s=200, c='#050505')
    
if __name__=="__main__":
    main()